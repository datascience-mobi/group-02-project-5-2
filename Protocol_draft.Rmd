---
title: "Cancer methylome analysis. Protocol group 2."
author: "Krank K., Kuchina M., Mendoza-Alcala A., Spatuzzi M."
output:
  pdf_document: default
  html_document: default
---

# Introduction

  DNA methylation is a well studied epigenetic mechanism which influences many processes in a cell including cell proliferation and differentiation but also development of common diseases such as cancer.   Studies show that methylomes of cancer cells have a different methylation pattern, especially in promoter regions, which indicates that gene expression is inhibited. (Stephen B Baylin 2005) Out team worked with the DNA methylation data collected from patients with acute myeloid leukemia (AML) and healthy people.The main goal of the project was to identify differentially methylated regions (DMRs) -  DNA sequences with different methylation status. This is the matter of scientific and clinical interest because reliable DMRs might help to discern diseased and healthy patients. In the following protocol we provide a comprehensive workflow of our project.     
 
# Data preprocessing

The data-set we had been working with contained a lot of irrelevant information. It was also partly incomplete - with many not available or missing values.  The coverage depth variated from unreliably low to suspiciously high (possibly resulted by PCR duplicates). A transfromation of heteroscedastic beta-values to homoscedastic M-values would be benefitial for direct applying of most statistical tests (Pan Du, Xiao Zhang, Chiang-Ching Huang, Nadereh Jafari, Warren A Kibbe, Lifang Hou and Simon M Lin 2010). Therefore, the data must have been cleaned and normalized to make further analysis possible.  

## Data clean-up

Goals: 

* remove regions with low or too high coverage 

* remove regions with high amount of missing values

* replace `NA`s with the mean value of the cohort

Work process:

1. Data importation and visualization 

  Data-set has been imported and separated into specific subgroups. 
  We defined `promoters_only` variable and splitted it into `beta` and `coverage` datasets for further operations. We also changed the sample names to simplify them.   

```{r data_import, echo=T} 

library(tictoc)
tic("total")

data <- readRDS(file = "AML_gran_list.RDS")
annotation <- read.csv(file="sample_annotation.csv")

promoters <- data$promoters;
promoters_only <- promoters[,-c(1:10)]

beta <- promoters_only[,c(1:18)]
coverage<- promoters_only[,c(19:36)]

PatientInfo <- annotation[,c(3,4,11,29,36)]
rownames(PatientInfo) <- c(
  paste("AML", 1:9, sep=""),
  paste("CON", 1:9, sep="")
)
```

  The general distribution of the coverage was visualized in the following plot:
  
```{r coverage_histo, echo=T}
par(mfrow=c(1,1))
coverage_tot_mean <- rowMeans(coverage) 
log_coverage_mean <- log10(coverage_tot_mean)

hist(log_coverage_mean, breaks = 250, xlim = c(0,5.5))
abline(v=quantile(log_coverage_mean, probs = c(seq(0.1, 1.0, by= 0.1))), col="red")
abline(v=quantile(log_coverage_mean, probs = c(0.05, 0.95), col="cyan"), lty=2)

quantile(coverage_tot_mean, probs=.05)
quantile(coverage_tot_mean, probs=.95)
```

  The green line identifies the 5% quantile while the red one is the 95% quantile. We can see that the distribution of our data is bimodal and does not follow the normal Gaussian distribution. We supposed that it might be a sign of bias-presence as a result of batch effects which will be discussed later.  
         
2.  Removing sex chromosomes.

  Several studies show that there are sex differences in methylation status. For example the second X-chromosome in females and the heterochromatic region of Y-chromosome are strongly methylated which does not correlate with any disease but is a normal occurence. (Bernardino J1, Lombard M, Niveleau A, Dutrillaux B. 2000, Jingyu Liu , Marilee Morgan, Kent Hutchison, Vince D. Calhoun 2010) Therefore, correction for sex is necessary and X- and Y-chromosomes must be removed to avoid gender bias.
  
```{r chr_xy_removal, echo=T, out.width="90%"}
cover_nox <- coverage[-which(promoters$Chromosome == "chrX"), ]
cover_noxy <- cover_nox[-which(promoters$Chromosome == "chrY"), ]
rm(coverage, cover_nox)
beta_nox <- beta[-which(promoters$Chromosome == "chrX"), ]
beta_noxy <- beta_nox[-which(promoters$Chromosome == "chrY"), ]
rm(beta, beta_nox)

print( (nrow(beta_noxy)*100) / nrow(promoters) )
```
  
  As shown above, the X- and Y-chromosomes represented about 4% of the information.


3. Coverage thresholds setting.

 3.1 The lower threshold
  
  Having the sex chromosomes removed we took the recommended value for the minimum coverage of 30x to set our lower threshold. (https://www.encodeproject.org/wgbs/)  
  
```{r lower_threshold, echo=T}

lower_threshold <- 30

cover_low_trimmed <- as.matrix(cover_noxy)
cover_low_trimmed_list <- which(cover_low_trimmed < lower_threshold)
beta_low_trimmed <- as.matrix(beta_noxy)
beta_low_trimmed[cover_low_trimmed_list] <- NA

beta_low_NA_rm <- beta_low_trimmed[-which(rowSums(is.na(beta_low_trimmed)) > 2), ]
which(rowSums(is.na(beta_low_NA_rm)) > 2) ## RESULT = 0

print( (nrow(beta_low_NA_rm)*100) / nrow(beta_noxy))
print( (nrow(beta_low_NA_rm)*100) / nrow(promoters_only))
```
  
 
 3.2 The upper threshold.
 
 First we decided to create a plot which would show the amount of values we would lose depending on the threshold settings. We used the `unlist` function in order to freely inspect the whole dataframe. We inspected the results for three quantiles: 75%, 95% and 100%.

``` {r upper_threshold_0, echo=T}
coverage_histo <- unlist(cover_noxy)
log_cover_histo <- unlist(log10(cover_noxy))
quantile(coverage_histo, probs=c(.75, .95, 1))
```

  We supposed that the upper threshold should have been located between 95% and 100% quantiles. Vectors `NAs_up` and `thresholds_up` were created. The following loop goes through the data-set in 200-steps and looks for the values which coverage is higher than the threshold. 

```{r upper_threshold_1, echo=T}
NAs_up <- c()
thresholds_up <- c()
for(i in seq(30, max(coverage_histo),200)){
  threshold <- i
  thresholds_up <- append(thresholds_up, threshold)
  Unreliable <- coverage_histo[which(coverage_histo > threshold)]
  N_Unreliable <- length(Unreliable)
  NAs_up <- append(NAs_up, N_Unreliable)
}

par(mfrow=c(1,1))
plot(x=log10(thresholds_up), y=NAs_up, type= "l", main = "Unreliable Data by threshold", xlab= "Threshold position", ylab = "Unreliable Data")
abline(v=quantile(log_coverage_mean, probs = c(seq(0.95, 1.0, by= 0.01))), col="green")
abline(v=quantile(log_coverage_mean, probs = 0.995), col="orange")
abline(v=quantile(coverage_tot_mean, probs = c(seq(0, 1.0, by= 0.1))), col="green")
quantile(coverage_histo, probs=.95)
```

  The plot shows how much information would be transformed to `NA`s depending on the position of the threshold. On this basis we decided that 95% quantile with the value 5888 could be our upper threshold. Soon enough we realized that it would be a wrong decision because we did not consider the fact that even the genes without `NA`s have to be removed if the same genes in the other cohort contain unreliable information. With every row we remove we also lose information in form of reliable values but at the same time values that have too low or loo high coverage are unreliable and therefore don't count as lost information.   
  So the following code demonstrates another approach to establish the upper threshold. We created a couple of empty vectors `rows_lost` and `thresholds_2NAs` and a zero vector `rows_lost_tot` containing a zero to make further addition operation (accumulation of values) possible. ` AML_position_above_limit <- which(df[i,c(1:9)] > threshold` would give us the position of values from AML cohort that would have coverage above the tested threshold. `CON_position_above_limit <- which(df[i,c(10:18)] > threshold)` executes the same command for the control group. All values above the tested threshold should be turned into `NA`s. To make the dataframe smaller (and the analysis faster) the loop creates a copy of the original dataframe and eliminates the rows that fulfill the criterium. The vector `rows_lost` helps to keep track on the amoint of removed rows.

 
```{r upper_threshold_2, echo=T}

df <- as.matrix(cover_noxy)
rows_lost_tot <- c(0)

for(threshold in quantile(df, probs=seq(1, 0.84, by=-0.002))){
  rows_lost <- c()
  
  AML_position_above_limit <- apply(df[,c(1:9)], 1, function(x){length(which(x > threshold))})
  CON_position_above_limit <- apply(df[,c(10:18)], 1, function(x){length(which(x > threshold))})
  
  rows_lost <- which(AML_position_above_limit > 2 | CON_position_above_limit > 2)
  rows_lost_tot <- append( rows_lost_tot, ( rows_lost_tot[length(rows_lost_tot)] + length(rows_lost) ) )
  
  if(length(rows_lost) > 0){
    df <- df[-rows_lost,]
  }
}

rows_lost_tot <- rows_lost_tot[-1]
thresholds <- quantile(df, probs=seq(1, 0.84, by=-0.002))

thresholds <- thresholds[-1]
rows_lost_tot <- rows_lost_tot[-1]

```

First we plotted the lost rows depending on the threshold. Each row with more than two `NA`s has been removed.

```{r lost_rows_plot, echo=T}

plot(x=thresholds, y=rows_lost_tot, type="b", main="Rows lost vs threshold") 
plot(x=thresholds[2:length(thresholds)], 
    y=rows_lost_tot[2:length(rows_lost_tot)],
     type="b", 
     main="Rows lost vs threshold (better visualization)") 
```

Then we plotted the percentage of information we would lose (the last two plots provide a better visualization without the last value).

```{r lost_rows_percent, echo=T}

rows_lost_per <- (rows_lost_tot*100) / nrow(cover_noxy)
plot(x=thresholds, y=rows_lost_per, type="b") 

plot(x=thresholds[2:21], y=rows_lost_tot[2:21], type="b")

rows_lost_per <- (rows_lost_tot*100) / nrow(cover_noxy)
plot(x=thresholds[c(2:21)], y=rows_lost_per[c(2:21)], type="b") 
abline(h=6, lty=2)

#Up until the removal of coverage values below 30, we've lost 10% of the information.
#Accordingly, the upper threshold should be chosen so that no more than 15% of the information (rows)
#get lost.

```

We decided not to lose more than 15% of information.


  3.3 Applying coverage thresholds to the data-set
  
  After establishing the upper and lower threshold positions we applied them to our data-set. New variable `promoters_qc` was created. All of the beta-values with coverage higher or lower than the thresholds were converted into `NA`.
  
```{r bad_coverage_to_NA, echo=F}
lower_threshold <- 30
upper_threshold <- quantile(coverage_histo, probs=0.995)

beta_trimmed <- as.matrix(beta_noxy)
cover_trimmed <- as.matrix(cover_noxy)
future_NAs <- which(cover_trimmed < lower_threshold | cover_trimmed > upper_threshold)
beta_trimmed[future_NAs] <- NA
```

  Genes with more than two `NA`s were first removed from both cancer and healthy cohort. We tolerated this amount of missing values because this way the data-sen would still be statistically significant. (Yiran Dong and Chao-Ying Joanne Peng, 2013) 

```{r no_more_than_2_NA, echo=F}
beta_AML <- beta_trimmed[,c(1:9)]
beta_con <- beta_trimmed[,c(10:18)]

NAs_AML <- rowSums(is.na(beta_AML))
NAs_con <- rowSums(is.na(beta_con))

AML <- cbind(beta_AML, NAs_AML)
con <- cbind(beta_con, NAs_con)

beta_qc <- cbind(AML, con)
beta_partly_cleaned <- beta_qc[-which(beta_qc[, 10] > 2 | beta_qc[, 20] > 2), ]
```

  And finally the `NA`s were replaced with the mean value of the cohort (healthy or diseased) so it wouldn't affect further statistical tests.
  
```{r rowmean_insteadof_NA, echo=T}
for (i in 1:nrow(beta_partly_cleaned)) {
  n <- beta_partly_cleaned[i,10]
  
  if (n==1 | n==2) {
    for (j in 1:9){
      if (is.na(beta_partly_cleaned[i,j])){
        beta_partly_cleaned[i,j] <- mean(beta_partly_cleaned[i,1:9], na.rm = T)
      }
    }
  }
  
  n <- beta_partly_cleaned[i,20]
  
  if (n==1 | n==2) {
    for (j in 11:19){
      if (is.na(beta_partly_cleaned[i,j])){
        beta_partly_cleaned[i,j] <- mean(beta_partly_cleaned[i,11:19], na.rm = T)
      }
    }
  }
}

beta_both_clean <- beta_partly_cleaned[,-c(10,20)]
```


## Data normalization

Goal:

* transfer beta-values to more statistically valid M-values

Work process:
  
The M-value is calculated as the log2 ratio of the intensities of methylated probe versus unmethylated probe so here we normalized our data by transforming the beta-values with the following loop.


```{r normalisation, echo=T}
for (j in 1:18) {
  for (i in 1:nrow(beta_both_clean)) {
    beta_both_clean[[i,j]] <- log2(beta_both_clean[i,j] / (1-beta_both_clean[i,j]) )
    
  }
}

promoters_normalized <- beta_both_clean
rm(beta_both_clean)

write.table(promoters_normalized, file="promoters_normalized.csv",sep=",",row.names=T) #Export table
promoters_normalized <- read.csv(file="promoters_normalized.csv")
```


# Statistical analysis

## Dimension reduction and PCA

High dimensional data provides many problems by analysing: from high demands on computing resources to impracticability of understanding the underlying structure and problems with visualization. There are many ways of dimensional reduction such as principal component analysis (PCA), heatmaps, t-SNE plots, multidimensional scaling etc. We focused on the PCA, one of the main methods to reduce data dimensionality while retaining most of the variaton in the data set. It is based on a process of identifying directions (or principal components) along wich the variation in the data is maximal. (Jolliffe, I.T. Principal Component Analysis Springer, New York, 2002, Ringner, M. 2008. What is principal component analysis?). 
By using a few components instead of thousands of variables we can plot samples and determine whether they can be grouped. We applyed PCA to check for batch effects and to look for any differences in methylation between cancer (AML) and control (con) group.
To use the `prcomp` function in R we must first turn our data into a matrix.

```{r, echo=T}
promoters_matrix <- as.matrix(promoters_normalized) 

class(promoters_matrix)
```

As PCA cannot be performed if there are beta values equal 0 or 1, so those values have been turned into `-Inf` and `Inf` respectively. Then we assigned exaggeratedly low (for `-Inf`) and high (for `Inf`) M values so we could run PCA without having our results affected from 0 or 1 beta values.

```{r, echo=T}
promoters_matrix_noInf <- promoters_matrix
promoters_matrix_noInf[which(promoters_matrix_noInf == Inf)] <- log2(0.999999999/(1-0.999999999))
promoters_matrix_noInf[which(promoters_matrix_noInf == -Inf)] <- -log2(0.999999999/(1-0.999999999))
```

The column names were turned into intuitive names and the PCA was run.

```{r, echo=T}
colnames(promoters_matrix_noInf) <- c(
  paste("AML", 1:9, sep=""),
  paste("CON", 1:9, sep=""))
  
promoters_pca <- prcomp(t(promoters_matrix_noInf), scale = TRUE)
```

So now `promoters_pca` contains three types of information: x, sdev and rotation. Here, `x` contains the principal components, `sdev` contains the information about how much standard deviation each component accounts for and `rotation` shows how much influence (loading scores) each gene has on each PC. Positive influences push points on a plot towards positive values and negative influences push the data points toward negative values.

```{r, echo=T}
#View(promoters_pca$x)
#View(promoters_pca$sdev)
#View(promoters_pca$rotation)
```

Quadratic value of `sdev` can be used to calculate how much variance each component accounts for. For a better comparison of the PCs we transformed that value into percents. 

```{r Scree_Plot, echo=T}
pca.var <- promoters_pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

plot(pca.var.per, x = c(1 :length(pca.var.per)), type = "b", main = "Scree Plot",  xlab= "Principal Component", ylab= "Percentage of variation" )
```

The first three PCs explain most of the variation within the data.
To plot the samples along the PCs we programmed several functions. We used `ggplot`s which let us easily combine or add different types of visualization components (or layers) together. 


```{r PCA_functions, echo=T}
library(ggplot2)
ggplot.pca.cellTypeShort <- function(i, j, npar=TRUE,print=TRUE){ 
  
  pca.data <- data.frame(Sample=rownames(promoters_pca$x), X=promoters_pca$x[,i], Y=promoters_pca$x[,j], PatientInfo$cellTypeShort )
  
  ggplot(data=pca.data, aes(x=X, y=Y, label="", group = PatientInfo.cellTypeShort)) +
    geom_text() +
    xlab(paste("PC_", i," ", pca.var.per[i], "%", sep =""))  +
    ylab(paste("PC_", j," ", pca.var.per[j], "%", sep =""))  +
    theme_bw()  + 
    ggtitle("My PCA Graph") + 
    geom_point(aes(shape=PatientInfo.cellTypeShort, color=PatientInfo.cellTypeShort))
  
}

ggplot.pca.cellTypeGroup <- function(i, j, npar=TRUE,print=TRUE){ 
  
  pca.data <- data.frame(Sample=rownames(promoters_pca$x), X=promoters_pca$x[,i], Y=promoters_pca$x[,j], PatientInfo$cellTypeGroup )
  
  ggplot(data=pca.data, aes(x=X, y=Y, label="", group = PatientInfo.cellTypeGroup)) +
    geom_text() +
    xlab(paste("PC_", i," ", pca.var.per[i], "%", sep =""))  +
    ylab(paste("PC_", j," ", pca.var.per[j], "%", sep =""))  +
    theme_bw()  + 
    ggtitle("My PCA Graph") + 
    geom_point(aes(shape=PatientInfo.cellTypeGroup, color=PatientInfo.cellTypeGroup))
  
}

ggplot.pca.FIRST_SUBMISSION_DATE <- function(i, j, npar=TRUE,print=TRUE){ 
  
  pca.data <- data.frame(Sample=rownames(promoters_pca$x), X=promoters_pca$x[,i], Y=promoters_pca$x[,j], PatientInfo$FIRST_SUBMISSION_DATE )
  
  ggplot(data=pca.data, aes(x=X, y=Y, label="", group = PatientInfo.FIRST_SUBMISSION_DATE)) +
    geom_text() +
    xlab(paste("PC_", i," ", pca.var.per[i], "%", sep =""))  +
    ylab(paste("PC_", j," ", pca.var.per[j], "%", sep =""))  +
    theme_bw()  + 
    ggtitle("My PCA Graph") + 
    geom_point(aes(shape=PatientInfo.FIRST_SUBMISSION_DATE, color=PatientInfo.FIRST_SUBMISSION_DATE))
  
}

ggplot.pca.BIOMATERIAL_PROVIDER <- function(i, j, npar=TRUE,print=TRUE){ 
  
  pca.data <- data.frame(Sample=rownames(promoters_pca$x), X=promoters_pca$x[,i], Y=promoters_pca$x[,j], PatientInfo$BIOMATERIAL_PROVIDER )
  
  ggplot(data=pca.data, aes(x=X, y=Y, label="", group = PatientInfo.BIOMATERIAL_PROVIDER)) +
    geom_text() +
    xlab(paste("PC_", i," ", pca.var.per[i], "%", sep =""))  +
    ylab(paste("PC_", j," ", pca.var.per[j], "%", sep =""))  +
    theme_bw()  + 
    ggtitle("My PCA Graph") + 
    geom_point(aes(shape=PatientInfo.BIOMATERIAL_PROVIDER, color=PatientInfo.BIOMATERIAL_PROVIDER))
  
}

ggplot.pca.DONOR_SEX <- function(i, j, npar=TRUE,print=TRUE){ 
  
  pca.data <- data.frame(Sample=rownames(promoters_pca$x), X=promoters_pca$x[,i], Y=promoters_pca$x[,j], PatientInfo$DONOR_SEX )
  
  ggplot(data=pca.data, aes(x=X, y=Y, label="", group = PatientInfo.DONOR_SEX)) +
    geom_text() +
    xlab(paste("PC_", i," ", pca.var.per[i], "%", sep =""))  +
    ylab(paste("PC_", j," ", pca.var.per[j], "%", sep =""))  +
    theme_bw()  + 
    ggtitle("My PCA Graph") + 
    geom_point(aes(shape=PatientInfo.DONOR_SEX, color=PatientInfo.DONOR_SEX))
  
}
```

So here are the plots to visualize the effect of cell types, date, biomaterial provider and donor's sex on the WGBS results.

```{r, echo=T}
ggplot.pca.cellTypeShort(1,2)

ggplot.pca.cellTypeGroup(1,2)

ggplot.pca.FIRST_SUBMISSION_DATE(1,2)

ggplot.pca.BIOMATERIAL_PROVIDER(1,2)

ggplot.pca.DONOR_SEX(1,2)
```


We can clearly see that PC1 separates the control group from the AML patients which all have values below the ones of the control group.
In PC2 two AML patients values are noticeably below the control groups values. Then they were separated into their own group.

```{r, echo=T}
ggplot.pca.cellTypeShort(1,3)

ggplot.pca.cellTypeGroup(1,3)

ggplot.pca.FIRST_SUBMISSION_DATE(1,3)

ggplot.pca.BIOMATERIAL_PROVIDER(1,3)

ggplot.pca.DONOR_SEX(1,3)
```

PC3 strongly separates AML patients at two extremes while still preserving the control group. The two groups are characterised by two different biomaterial providers, except for one patient. We may concern that the biomaterial provider influenced the result of the sequencing.

##Kruskal-Wallis test

This test helps to determine whether the samples originate from the same distribution or not. It is a non-parametric method and the Kruskal Wallis test does not assume a normal distribution of the residuals. Assuming that all groups are identically shaped and have a scaled distribution, the null hypothesis is that the medians of all groups are equal, and the alternative hypothesis is that at least one population median of one group is different from the population median of at least one other group.
We created a dataframe with possible sources of variation and the PCs to inspect them closely.

```{r kurskal_0, echo=T}
pca_promoters <- promoters_pca$x
sample_annotation <- read.csv(file="sample_annotation.csv")
PatientInfo_kruskal <- sample_annotation[,c(3,28,29,33)]

kruskal_df <-cbind(PatientInfo_kruskal,pca_promoters)
colnames(kruskal_df) <- c(1:22)
```

We applied the Kruskal-Wallis test on PC 1 to 5 (columns 5 to 10 in the dataframe) to inspect if there is any bias provided by different cell type (biological bias, not batch), disease (expactable), donor-ID and biomaterial provider (columns 1 to 4 respectively). For that a matrix of p-values was created with PCs as columns and batch-effect sources as rows. 

```{r kurskal_1, echo=T}

heatmap_kruskal <- matrix(nrow=4,ncol=5)

for (j in 1:4){
  for (i in 5:9) {
    heatmap_kruskal[j,i-4] <- kruskal.test(kruskal_df[,i] ~ kruskal_df[,j], data = kruskal_df)$p.value
    
  }
}
```

Then a similar matrix with statistic value was created.

```{r kurskal_2, echo=T}

kruskal_statistic <- matrix(nrow=4,ncol=5)

for (j in 1:4){
  for (i in 5:9) {
    kruskal_statistic[j,i-4] <- kruskal.test(kruskal_df[,i] ~ kruskal_df[,j], data = kruskal_df)[["statistic"]][["Kruskal-Wallis chi-squared"]]
    
  }
}
```

##Wilcoxon rank-sum test

The Wilcoxon rank-sum tets is an alternative to the two-sample t-test.  

```{r wilcoxon, echo=T}
gender <- sample_annotation[,36]
wilcoxon_df <- cbind(gender,pca_promoters)
colnames(wilcoxon_df) <- c(1:19)

# with this code wa have a matrix with the p.values 
heatmap_wilcoxon <- matrix(nrow=1,ncol=5)

for (i in 2:6) {
  heatmap_wilcoxon[1,i-1] <- wilcox.test(wilcoxon_df[,i] ~ wilcoxon_df[,1], data = wilcoxon_df)$p.value
  
}
# and here the statistic_values

wilcoxon_statistic <- matrix(nrow=1,ncol=5)


for (i in 2:6) {
  wilcoxon_statistic[1,i-1] <- wilcox.test(wilcoxon_df[,i] ~ wilcoxon_df[,1], data = wilcoxon_df)[["statistic"]][["W"]]
  
}

```

##Permutation test

Permutation test (or a re-randomization test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under rearrangements of the labels on the observed data points. 
First we calculate Pearsons r correlation coefficient and it's significance tested by parametric t-test.

```{r perm_test, echo=T}
age <- sample_annotation[,c(34)]
age_separated <- strcapture("(.*)-(.*)", as.character(age), data.frame(type_1 = "", type_2 = ""))
age <- age_separated[,1]
correlation_df <- cbind(age, pca_promoters)
heatmap_correlation <- matrix(nrow=1,ncol=5)


cor.perm <- function (x,y, nperm = 499)
{
  r.obs <- cor (x = x, y = y)
  P.par <- cor.test (x = x, y = y)$p.value
  #  r.per <- replicate (nperm, expr = cor (x = x, y = sample (y)))
  r.per <- sapply (1:nperm, FUN = function (i) cor (x = x, y = sample (y)))
  r.per <- c(r.per, r.obs)
  hist (r.per, xlim = c(-1,1))
  abline (v = r.obs, col = 'red')
  P.per <- sum (abs (r.per) >= abs (r.obs))/(nperm + 1) 
  return (P.per = P.per)
}

for (i in 2:6){
  heatmap_correlation[1, i-1] <- cor.perm(x=correlation_df[,1], y=correlation_df[,i])
  
  
}

correlation_statistics <- matrix(nrow=1, ncol=5)

cor.perm_statistics <- function (x,y, nperm = 499)
{
  r.obs <- cor (x = x, y = y)
  P.par <- cor.test (x = x, y = y)[["statistic"]][["t"]]
  #  r.per <- replicate (nperm, expr = cor (x = x, y = sample (y)))
  r.per <- sapply (1:nperm, FUN = function (i) cor (x = x, y = sample (y)))
  r.per <- c(r.per, r.obs)
  hist (r.per, xlim = c(-1,1))
  abline (v = r.obs, col = 'red')
  P.per <- sum (abs (r.per) >= abs (r.obs))/(nperm + 1) 
  return (P.per = P.per)
}

for (i in 2:6){
  correlation_statistics[1, i-1] <- cor.perm_statistics(x=correlation_df[,1], y=correlation_df[,i])
  
  
}

```

##Heatmap

We created three 5 x 300 pixels (300 pixels per inch) PNG files for the heatmaps. 

```{r heatmap, echo=T}
heatmap <- rbind(heatmap_kruskal,heatmap_wilcoxon,heatmap_correlation)
colnames(heatmap) <- c("PC1","PC2","PC3","PC4","PC5")
rownames(heatmap) <- c("cellType","Disease","Biomaterial-provider","Donor-ID","gender","Age")

statistics <- rbind(kruskal_statistic,wilcoxon_statistic,correlation_statistics)
colnames(statistics) <- c("PC1","PC2","PC3","PC4","PC5")
rownames(statistics) <- c("cellType","Disease","Biomaterial-provider","Donor-ID","gender","Age")


library(gplots)

my_palette <- colorRampPalette(c("red", "yellow"))(n = 3)
col_breaks <- c(seq(0,0.01,length=2),  
                seq(0.0105, 1,length=2)
) 

par(mar=c(1,1,1,1))
heatmap.2(heatmap, 
          main = "Significance of effects on data",
          density.info="none",
          trace="none",
          lwid= c(2,5),
          margins =c(6,10),
          col=my_palette,
          breaks=col_breaks,
          dendrogram = "none" ,
          Colv = "NA"
)


my_palette <- colorRampPalette(c("red", "yellow"))(n = 3)
col_breaks <- c(seq(0,0.05,length=2),  
                seq(0.0505, 1,length=2)
) 

par(mar=c(1,1,1,1))
heatmap.2(heatmap, 
          main = "Significance of effects on data",
          density.info="none",
          trace="none",
          lwid= c(2,5),
          margins =c(6,10),
          col=my_palette,
          breaks=col_breaks,
          dendrogram = "none" ,
          Colv = "NA"
)

my_palette <- colorRampPalette(c("red", "yellow"))(n = 3)
col_breaks <- c(seq(0,0.05,length=2),  
                seq(0.0505, 1,length=2)
) 


par(mar=c(1,1,1,1))
heatmap.2(statistics, 
          main = "Significance of effects on data",
          density.info="none",
          trace="none",
          lwid= c(2,5),
          margins =c(6,10),
          col=my_palette,
          breaks=col_breaks,
          dendrogram = "none" ,
          Colv = "NA"
)

toc()
```

# Interpretation

##Logistic regression